---
title: "Ethics: Data & Power"
description: |
 Examining the Role of Ethics in Data Science Through an Algorithm That Predicts Child Neglect 
  
format: html

author: Emma N.
date: November 12, 2025
---

Pennsylvania’s Allegheny County, in which Pittsburgh is situated, has implemented a predictive tool for child neglect detection since August of 2016 \[1\]. The algorithm, called the Allegheny Family Screening Tool (AFST) is utilized by the child welfare agency in the county to assess the gravity of received reports of child neglect and to make decisions about which families should be further investigated \[2\]. Using data on family Medicaid and substance abuse records, as well as jail and probation history, the AFST assigns risk scores to flagged children which, in turn, are used to decide whether a full investigation (and potentially foster-care placement) is advisable \[1\]. More specifically, the tool is designed to predict how likely it is that a child will need to be removed from their home in the long-term \[2\].

As such, the algorithm represents a somewhat automated approach to a decision that would usually be made on an individual basis—data science is utilized to predict risk of child neglect based on contextual factors. The approach has plausible benefits but simultaneously raises various ethical concerns about privacy and bias. One such ethical concern is the disproportionate distribution of risk scores across race. Even though the AFST doesn’t directly use any data on race, the measures it does use skew results due to historical inequalities and over-surveillance of poor, Black, and marginalized families \[1\]. Additionally, there was a technical glitch in the tool for more than two years where some children got inaccurately high or low scores \[1\]. Ethical concerns extend past the output of the algorithm itself and are founded on the way in which the AFST results are used. Overall, there isn’t much transparency—and much less consent—about the fact that data from multiple government systems are being analyzed to affect a family’s child welfare investigation \[1\]. As a result, the combination of algorithmic mistakes, built-in biases, and insufficient transparency prompt an ongoing debate about the ethics of the AFST \[2\]. In the following paragraphs, I will assess the extent to which the use of the AFST in Allegheny County adheres to or diverges from four different components of the Data Values and Principles Manifesto \[3\].

The first principle in the Manifesto reads “Use data to improve life for our users, customers, organizations, and communities” \[3\]. This principle is important for ensuring that data science is used for making a positive impact, rather than a bad one. The issue is, of course, that what is considered to be “positive” can often be subjective. The interests of customers, for example, often seem to be at odds with the interests of sellers. Whether this principle adopts a utilitarian view of the greatest possible good is unclear, but it may be said that the AFST certainly seems to aim for the improvement of the life of the communities it interacts with. After all, it was designed to help the child welfare agency in Allegheny County make more reliably accurate decisions about whether a child is actually in danger of neglect \[1\]. By expediting the process of evaluating reports of child neglect, the agency is able to process more reports at a faster rate, so that it can intervene sooner and more reliably. A comprehensive evaluation of the AFST by Stanford’s Professor Goldhaber-Fiebert found that “Implementation of the AFST and associated policies increased accuracy for children screened-in for investigation” \[4\].

The 7th principle in the Manifesto highlights the need to “recognize and mitigate bias in ourselves and in the data we use” \[3\]. There are countless forms of biases that can occur in collected data, which consequently skew the results drawn from data. Biased results can have severe and devastating repercussions, from simply supporting incorrect opinions to drastically affecting the communities that the data implicates. Completely eliminating bias from data is often nearly impossible, but measures should always be taken to ensure that information is interpreted fairly and used ethically. This, it can be argued, is not necessarily the case for the AFST. Researchers at Carnegie Mellon University, for example, found that during early years (Aug 2016–May 2018) the tool flagged a higher percentage of Black children for “mandatory” investigations (32.5%) than white children (20.8%) \[1\]. Such skews in the data could have led to disproportionate quantities of Black families being separated from their children, a terrible circumstance if not properly justified.

The 10th principle of the Manifesto reads as follows: “Respect and invite fair criticism while promoting the identification and open discussion of errors, risks, and unintended consequences of our work” \[3\]. This is a key aspect of ethical data science, since errors can be missed by even the most rigorous of evaluations. Being open to criticisms and the identification of errors from others is vital to ensuring that the conclusions drawn from the data are as ethical as possible. Allegheny County’s child welfare agency seems to do a relatively good job at this. On their website, which contains a comprehensive description of the AFST and its use, they include links and responses to various criticisms of their algorithm \[2\]. Rather than ignoring or shutting down any pessimistic or critical views of their program, they engage with them in a meaningful and fair manner. In a statement released after the publication of the article Associated Press (AP) article “An algorithm that screens for child neglect raises concerns”, they provide detailed responses that address the main criticisms described in the article \[5\].

Finally, I will go back to the 9th principle of the Manifesto, which cautions data scientists to “consider carefully the ethical implications of choices \[they\] make when using data, and the impacts of \[their\] work on individuals and society” \[3\]. As mentioned above, the misuse of data can have grave consequences, so it is vital that data scientists are aware of the ethical considerations that accompany their work. Despite the concerns raised above, it seems that the Allegheny County child welfare agency and the developers of the AFST seem to be well aware of the gravity of their roles and decisions. One of the most important aspects of their approach, one might argue, is the fact that they only use the AFST algorithm as a support for decision-making, and is exclusively used in hotline call screenings—not family court or removal decisions \[5\]. This is described in detail in the statement mentioned above, in which it is stated that “the AFST was not designed, nor has there ever been a discussion of using it, to autonomously make \[…\] screening decisions” \[5\]. The algorithm is purely meant to assist with decisions, not to make them independently, which demonstrates that Allegheny County is not failing to take the gravity of their actions seriously.

All of the concerns discussed above matter because the well-being of thousands of families and their children across the county of Allegheny (and across the nation if such algorithms are adopted in other regions) are at stake. The data science principles addressed provide a small insight into why it is vital to utilize data carefully and ethically—a concern that is only exacerbated by the sensitive situation that is child neglect. The ethical violations that are occurring seem to be largely unintentional (not in the interest of profit, for example), but might be the result of cutting corners in the interest of efficiency. Such shortcomings might provide small benefits to the child welfare agency but can have devastating consequences for the affected families and children.

Bibliography:

\[1\] Ho, S., & Burke, G. (2022, April 29). *An algorithm that screens for child neglect raises concerns*. Pulitzer Center. <https://pulitzercenter.org/stories/algorithm-screens-child-neglect-raises-concerns>

\[2\] Allegheny County Department of Human Services. (n.d.). *Allegheny Family Screening Tool: Predictive risk modeling in child welfare in Allegheny County*. Retrieved November 12, 2025, from [https://www.alleghenycounty.us/Services/Human-Services-DHS/DHS-News-and-Events/Accomplishments-and-Innovations/Allegheny-Family-Screening-Tool](#0)

\[3\] Data Practices. (n.d.). *Manifesto for Data Practices: Data values and principles*. Retrieved November 12, 2025, from [https://datapractices.org/manifesto/](https://datapractices.org/manifesto/?utm_source=chatgpt.com)

\[4\] Goldhaber-Fiebert, J. D., & Prince, L. (2019, March). *Impact evaluation of a predictive risk modeling tool for Allegheny County’s child welfare office* (Report No. 16-ACDHS-26). Allegheny County Department of Human Services. [https://www.alleghenycounty.us/files/assets/county/v/1/services/dhs/documents/allegheny-family-screening-tool/impact-evaluation-from-16-acdhs-26_predictiverisk_package_050119_final-6.pdf](https://www.alleghenycounty.us/files/assets/county/v/1/services/dhs/documents/allegheny-family-screening-tool/impact-evaluation-from-16-acdhs-26_predictiverisk_package_050119_final-6.pdf?utm_source=chatgpt.com)

\[5\] Allegheny County Department of Human Services. (2022, May 9). *DHS response to AP article “An algorithm that screens for child neglect raises concerns”* \[PDF\]. [https://www.alleghenycounty.us/files/assets/county/v/1/services/dhs/documents/allegheny-family-screening-tool/dhs-response-to-ap-article_algorithm-that-screens-for-child-neglect.pdf](https://www.alleghenycounty.us/files/assets/county/v/1/services/dhs/documents/allegheny-family-screening-tool/dhs-response-to-ap-article_algorithm-that-screens-for-child-neglect.pdf?utm_source=chatgpt.com)
